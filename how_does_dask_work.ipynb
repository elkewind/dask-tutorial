{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does Dask work? Why should I use Dask?\n",
    "\n",
    "Dask allows you to scale up your most favorite functions such as Pandas, NumPy, and Scikit-learn to use on Big Data. \n",
    "\n",
    "Arrays, Dataframes, and semi-structured/unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby take up a great deal of RAM when processed directly with Pandas, Numpy, and Scikit-learn. These functions are not great at memory management so they front-load their work which puts stress on your RAM and causes it to slow down when fed in too much data.\n",
    "\n",
    "Dask resolves this by efficiently breaking down your data and feeding it to the RAM in bite sized pieces, ultimately combining it back into readable and usable data once complete.\n",
    "\n",
    "Dask has three main \"collections\" called Dask Array, Dask Dataframe, and Dask-ML (Dask Bag) which are alternatives to Pandas DataFrame, Numpy Array and Scikit-learn. \n",
    "\n",
    "Below, we will be descibing how Dask manages each of these collections to make them more efficient.\n",
    "\n",
    "## Dask Array\n",
    "A Dask array is made up of many smaller n-dimensional Numpy arrays and uses a blocked algorithm to allow computation on arrays that are larger than your available RAM memory. During an Array operation, Dask translates the array operation into a task graph which breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk at the same time. This is called parallel computing. Results from each chunk are combined to produce the final output. \n",
    "\n",
    "## Dask Dataframe\n",
    "A Dask DataFrame maintains the familiar Pandas code structure and language, making it easy for Pandas users to scale up DataFrame workloads, just replace ```pd``` with ```dd```. During a DataFrame operation, Dask creates a task graph and breaks down the dataframe into smaller parts that reduces memory footprint and increases RAM efficiency by sharing and deleting intermediate results while computing.\n",
    "\n",
    "## Dask-ML (Dask Bag)\n",
    "Dask Bag is an unordered collection of repeated objects which is a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How does it schedule it all?\n",
    "\n",
    "Dask’s high and low-level collections create a directed acyclic graph of tasks,[22] which represents the relationship between computation tasks. A node in a task graph represents a Python function that performs a unit of computation and an edge represents the data dependency between the upstream and downstream task. After a task graph is generated, the task scheduler manages the workflow based on the given task graph by assigning tasks to workers in a manner that improves parallelism and respects the data dependencies.\n",
    "\n",
    "Dask provides two families of schedulers: single-machine scheduler and distributed scheduler.\n",
    "Single-machine scheduler\n",
    "\n",
    "A single-machine scheduler is the default scheduler which provides basic features on local processes or thread pool and is meant to be used on a single machine. It is simple and cheap to use but does not scale.\n",
    "\n",
    "Local threads\n",
    "    A threaded scheduler leverages Python’s concurrent.futures.ThreadPoolExecuter to execute computations. It has a low memory footprint and does not require any setup. As all the computations occur in the same process, threaded schedulers incur minimal task overhead and no cost from transfer of data between tasks. Due to Python’s Global Interpreter Lock, local threads provide parallelism only when the computation is primarily non-Python code, which is the case for Pandas DataFrame, Numpy arrays or other Python/C/C++ based projects.\n",
    "Local process\n",
    "    A multiprocessing scheduler leverages Python’s concurrent.futures.ProcessPoolExecutor to execute computations. Tasks and its dependencies are transferred from the main process to a local process, executed, and the results are transferred back to the main process. This allows bypassing of issues with Python’s Global Interpretable Lock and provides parallelism for computation tasks with primarily Python code. However, transferring data between the main and local processes degrades performance, especially in cases when the size of data transferred is large.\n",
    "Single thread\n",
    "    A single threaded scheduler executes computation with no parallelism. It is used for debugging purposes.\n",
    "\n",
    "Distributed scheduler\n",
    "Dask’s distributed scheduler[23] can be set up on a local machine or scale out on a cluster. Dask can work with a number of different resource managers, such as Hadoop YARN, Kubernetes, or PBS, Slurm, SGD and LSF for High Performance Computing (HPC) clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where and how can you use Dask?\n",
    "## Use cases: https://stories.dask.org/en/latest/\n",
    "\n",
    "Retail\n",
    "\n",
    "    Walmart uses Dask for forecasting the demand for 500,000,000 store-item combinations. To provide in-demand items in sufficient quantities at all their outlets, they need to run huge computations. Using RAPIDS and XGBoost, supported by Dask, they reached 100x acceleration.[10]\n",
    "    Blue Yonder uses Dask to process terabytes of data on a daily basis.[33] They can write Pandas-like code in Dask, which can then be pushed directly to production. This helps keep their feedback cycles short and waste low.\n",
    "    Grubhub uses Dask[2][34] alongside TensorFlow for pre-processing and ETL. Dask allows them to continue working in Python and get the functionalities they need.\n",
    "\n",
    "Life sciences\n",
    "\n",
    "Dask is used for high resolution, 4-dimensional, cellular imagery by Harvard Medical School, Howard Hughes Medical Institute, Chan Zuckerberg Initiative, and the UC Berkeley Advanced Bioimaging Center.[7] They record the evolution and movements of a 3-dimensional cell over time, in maximum detail. This generates large amounts of data that is difficult to analyze with traditional methods. Dask helps them scale their data analysis workflows with its familiar API that resembles NumPy, Pandas, and Scikit-learn code. Dask is also used at the Novartis Institute for Biomedical Research to scale machine learning prototypes.\n",
    "Finance industry\n",
    "\n",
    "    Capital One[35] uses Dask to accelerate ETL and ML pipelines\n",
    "    Barclays[36] for financial system modeling\n",
    "\n",
    "Geophysical sciences\n",
    "\n",
    "Dask is used in Climate Science, Energy, Hydrology, Meteorology, and Satellite Imaging by companies such as NASA, LANL, PANGEO:[37] Earth Science and the UK Meteorology Office.[38]\n",
    "\n",
    "With Dask, oceanographers can produce massive simulated datasets of the earth’s oceans and researchers can look at large seismology datasets from sensors around the world, collect a large number of observations from satellites and weather stations, and run big simulations.\n",
    "Software libraries\n",
    "\n",
    "Dask is integrated into many libraries, such as Pangeo [39] and xarray;[19] time series software like Prophet [40] and tsfresh;[41] ETL/ML software like Scikit-learn,[42] RAPIDS,[43] and XGBoost;[28] workflow management tools like Apache Airflow[44] and Prefect.[45] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "\n",
    "## https://en.wikipedia.org/wiki/Dask_(software)#High-Level_collections\n",
    "\n",
    "## https://stories.dask.org/en/latest/\n",
    "\n",
    "## https://earth-env-data-science.github.io/lectures/dask/dask_arrays.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('mikeys_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "579423154b4eaee7bfe5a430418673f411175c3f3abce33f4969df0aa8270f69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
