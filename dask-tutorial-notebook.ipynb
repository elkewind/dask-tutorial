{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Tutorial\n",
    "### By Michelle Lam, Elke Windschitl, & Michael Zargari for EDS-217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutoriol on how to use the Python library Dask. Dask is a tool to scale data libraries in Python such as Numpy, Pandas, and Scikit-learn. This means that smaller libraries can be scaled to used on big data. Dask can be deployed anywhere, so users can start on a laptop and scale up to cloud computing.\n",
    "\n",
    "Visit https://www.dask.org/ to learn more about Dask.\n",
    "\n",
    "## Why use Dask?\n",
    "Dask can be used when working with big data sets. Environmental data scientists may encounter big data sets frequently. But why do we need to use dask? \n",
    "\n",
    "*\"Big data is data sets that are so voluminous and complex that traditional data processing application software are inadequate to deal with them.\"* (Wikipedia)\n",
    "\n",
    "Libraries such as Numpy and Pandas aren't built to handle big data sets. Dask works with Numpy and Pandas under the hood to run familiar functions on large sets of data. The maintainers of Dask are the same maintainers of Numpy and Pandas.\n",
    "\n",
    "<img src=\"./dask-screenshot.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Dask work?\n",
    "\n",
    "Arrays, Dataframes, and semi-structured/unstructured data such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby take up a great deal of RAM when processed directly with Pandas, Numpy, and Scikit-learn. These functions are not great at memory management so they front-load their work which puts stress on your RAM and causes it to slow down when fed in too much data.\n",
    "\n",
    "Dask resolves this by efficiently breaking down your data and feeding it to the RAM in bite sized pieces, ultimately combining it back into readable and usable data once complete.\n",
    "\n",
    "Dask has three main \"collections\" called Dask Array, Dask Dataframe, and Dask-ML (Dask Bag) which are alternatives to Pandas DataFrame, Numpy Array and Scikit-learn. \n",
    "\n",
    "Below, we will be descibing how Dask manages each of these collections to make them more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Different Parts to the Dask project\n",
    "### 1. Dask Collections (\"core-library\")\n",
    "- **High-level collections**: mimic NumPy, lists, and pandas, but can operate in parallel on datasets that don't fit into memory\n",
    "    - Array\n",
    "    - Bag\n",
    "    - DataFrame\n",
    "- **Low-level collections**: give you finer control in building custom parallel and distributed computations\n",
    "    - Delayed\n",
    "    - Futures\n",
    "\n",
    "### 2. Dask Cluster\n",
    "Dask uses a distributed scheduler, which exists in the context of a Dask cluster.\n",
    "\n",
    "Structure of a dask cluster:\n",
    "\n",
    "<img src=\"./dask_cluster_img.png\" width = \"600\"/>\n",
    "\n",
    "### 3. Dask Ecosystem\n",
    "The Dask ecosystem connects several adiitional open source projects that provide different mechanisms for deploying Dask clusters. \n",
    "\n",
    "**This tutorial will focus on using the high-level collections of Array, Bag, and DataFrame.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Collection\n",
    "### Dask Array\n",
    "A Dask array is made up of many smaller n-dimensional Numpy arrays and uses a blocked algorithm to allow computation on arrays that are larger than your available RAM memory. During an Array operation, Dask translates the array operation into a task graph which breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk at the same time. This is called parallel computing. Results from each chunk are combined to produce the final output. \n",
    "\n",
    "### Dask Dataframe\n",
    "A Dask DataFrame maintains the familiar Pandas code structure and language, making it easy for Pandas users to scale up DataFrame workloads, just replace ```pd``` with ```dd```. During a DataFrame operation, Dask creates a task graph and breaks down the dataframe into smaller parts that reduces memory footprint and increases RAM efficiency by sharing and deleting intermediate results while computing.\n",
    "\n",
    "### Dask-ML (Dask Bag)\n",
    "Dask Bag is an unordered collection of repeated objects which is a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "To get started using Dask in Python, check to see if it is already on your laptop. Dask is included with Anaconda, so it may have installed when you downloaded Anaconda.\n",
    "\n",
    "```\n",
    "# To check for dask, try importing. \n",
    "import dask\n",
    "```\n",
    "\n",
    "If you do not have dask, Python will let you know. If you get a message saying 'No module named dask', you will need to install dask using conda in the command line before you import.\n",
    "\n",
    "```\n",
    "# To install dask use conda in your Powershell command line:\n",
    "conda install dask\n",
    "```\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to install xarray to use large data sets with dask\n",
    "\n",
    "```\n",
    "# Install xarray in your Powershell command line:\n",
    "conda install -c conda-forge xarray dask netCDF4 bottleneck\n",
    "```\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('eds217')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01298427d7509dc791212f5580d72e2a69e14f859fb9b9009471feff7d08282d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
