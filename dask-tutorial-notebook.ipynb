{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Tutorial\n",
    "### By Michelle Lam, Elke Windschitl, & Michael Zargari for EDS-217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutoriol on how to use the Python library Dask. Dask is a tool to scale data libraries in Python such as Numpy, Pandas, and Scikit-learn. This means that smaller libraries can be scaled to used on big data. Dask can be deployed anywhere, so users can start on a laptop and scale up to cloud computing.\n",
    "\n",
    "Visit https://www.dask.org/ to learn more about Dask.\n",
    "\n",
    "## Why use Dask?\n",
    "Dask can be used when working with big data sets. Environmental data scientists may encounter big data sets frequently. But why do we need to use dask? \n",
    "\n",
    "*\"Big data is data sets that are so voluminous and complex that traditional data processing application software are inadequate to deal with them.\"* (Wikipedia)\n",
    "\n",
    "Libraries such as Numpy and Pandas aren't built to handle big data sets. Dask works with Numpy and Pandas under the hood to run familiar functions on large sets of data. The maintainers of Dask are the same maintainers of Numpy and Pandas.\n",
    "\n",
    "<img src=\"./dask-screenshot.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Dask work?\n",
    "\n",
    "Arrays, Dataframes, and semi-structured/unstructured data such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby take up a great deal of RAM when processed directly with Pandas, Numpy, and Scikit-learn. These functions are not great at memory management so they front-load their work which puts stress on your RAM and causes it to slow down when fed in too much data.\n",
    "\n",
    "Dask resolves this by efficiently breaking down your data and feeding it to the RAM in bite sized pieces, ultimately combining it back into readable and usable data once complete.\n",
    "\n",
    "Dask has three main \"collections\" called Dask Array, Dask Dataframe, and Dask-ML (Dask Bag) which are alternatives to Pandas DataFrame, Numpy Array and Scikit-learn. \n",
    "\n",
    "Below, we will be descibing how Dask manages each of these collections to make them more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Different Parts to the Dask project\n",
    "### 1. Dask Collections (\"core-library\")\n",
    "- **High-level collections**: mimic NumPy, lists, and pandas, but can operate in parallel on datasets that don't fit into memory\n",
    "    - Array\n",
    "    - Bag\n",
    "    - DataFrame\n",
    "- **Low-level collections**: give you finer control in building custom parallel and distributed computations\n",
    "    - Delayed\n",
    "    - Futures\n",
    "\n",
    "### 2. Dask Cluster\n",
    "Dask uses a distributed scheduler, which exists in the context of a Dask cluster.\n",
    "\n",
    "Structure of a dask cluster:\n",
    "\n",
    "<img src=\"./dask_cluster_img.png\" width = \"600\"/>\n",
    "\n",
    "### 3. Dask Ecosystem\n",
    "The Dask ecosystem connects several adiitional open source projects that provide different mechanisms for deploying Dask clusters. \n",
    "\n",
    "**This tutorial will focus on using the high-level collections of Array, Bag, and DataFrame.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Collection\n",
    "### Dask Array\n",
    "A Dask array is made up of many smaller n-dimensional Numpy arrays and uses a blocked algorithm to allow computation on arrays that are larger than your available RAM memory. During an Array operation, Dask translates the array operation into a task graph which breaks up large Numpy arrays into multiple smaller chunks, and executes the work on each chunk at the same time. This is called parallel computing. Results from each chunk are combined to produce the final output. \n",
    "\n",
    "### Dask Dataframe\n",
    "A Dask DataFrame maintains the familiar Pandas code structure and language, making it easy for Pandas users to scale up DataFrame workloads, just replace ```pd``` with ```dd```. During a DataFrame operation, Dask creates a task graph and breaks down the dataframe into smaller parts that reduces memory footprint and increases RAM efficiency by sharing and deleting intermediate results while computing.\n",
    "\n",
    "### Dask-ML (Dask Bag)\n",
    "Dask Bag is an unordered collection of repeated objects which is a hybrid between a set and a list. Dask Bag is used to parallelize computation of semi-structured or unstructured data, such as JSON records, text data, log files or user-defined Python objects using operations such as filter, fold, map and groupby. Dask Bags can be created from an existing Python iterable or can load data directly from text files and binary files in the Avro format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "To get started using Dask in Python, check to see if it is already on your laptop. Dask is included with Anaconda, so it may have installed when you downloaded Anaconda.\n",
    "\n",
    "```\n",
    "# To check for dask, try importing. \n",
    "import dask\n",
    "```\n",
    "\n",
    "If you do not have dask, Python will let you know. If you get a message saying 'No module named dask', you will need to install dask using conda in the command line before you import.\n",
    "\n",
    "```\n",
    "# To install dask use conda in your Powershell command line:\n",
    "conda install dask\n",
    "```\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all of Dask run:\n",
    "import dask\n",
    "\n",
    "# To use Dask Data Frames run:\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, Dask is made to handle large data sets. However, for the purposes of this class, we will be using a smaller data set as an example. \n",
    "\n",
    "Here we use data from: *Ecological and social Interactions in urban parks: bird surveys in local parks in the central Arizona-Phoenix metropolitan area* by Warren, Paige S.; Research Assistant Professor; University of Massachusetts-Amherst\n",
    "Kinzig, Ann; Assistant Professor; Arizona State University\n",
    "Martin, Chris A; Associate Professor; Arizona State University\n",
    "Machabee, Louis; Global Institute of Sustainability, Arizona State University\n",
    "\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object that is a link to data\n",
    "birds_link = 'https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-cap.256.10&entityid=53edaa7a0e083013d9bf20322db1780e'\n",
    "\n",
    "# Read in the data using dd.read_csv()\n",
    "birds_dask = dd.read_csv(birds_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ```dd.read_csv()``` is similar to ```pd.read_csv()```. Functions that are used in Pandas can be used in Dask. Compare reading in a data frame in dask with reading in a data frame with Pandas.\n",
    "\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create the same data frame using pandas\n",
    "birds_pandas = pd.read_csv(birds_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "...........................................................................\n",
      "<bound method NDFrame.head of        survey_id site_id species_id distance  bird_count  \\\n",
      "0            144    LI-S       HOSP     5-10         4.0   \n",
      "1            145    LI-W       HOSP    20-40        10.0   \n",
      "2            145    LI-W       AUWA    20-40         2.0   \n",
      "3            145    LI-W       RODO       FT         2.0   \n",
      "4            145    LI-W       GTGR      >40         2.0   \n",
      "...          ...     ...        ...      ...         ...   \n",
      "40420       2001    WS-C       INDO    10-20         6.0   \n",
      "40421       2001    WS-C       VERD    10-20         1.0   \n",
      "40422       2001    WS-C       VERD    20-40         1.0   \n",
      "40423       2001    WS-C       RSFL    10-20         1.0   \n",
      "40424       2001    WS-C       GTGR       FT         3.0   \n",
      "\n",
      "                                     notes  seen  heard direction  \n",
      "0                                      NaN     1      1        NE  \n",
      "1                                      NaN     0      1        E   \n",
      "2                                      NaN     0      1        SE  \n",
      "3                                      NaN     1      0        E   \n",
      "4                                      NaN     0      1        NE  \n",
      "...                                    ...   ...    ...       ...  \n",
      "40420                                  NaN     1      0        SE  \n",
      "40421                                  NaN     1      1        SE  \n",
      "40422                                  NaN     1      1        SW  \n",
      "40423  not same individual as previous one     1      0        SW  \n",
      "40424                                  NaN     1      0        E   \n",
      "\n",
      "[40425 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Look at you pandas output\n",
    "print(type(birds_pandas))\n",
    "print('...........................................................................')\n",
    "print(birds_pandas.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare these outputs with your dask data frame. What is different?\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _Frame.head of Dask DataFrame Structure:\n",
      "              survey_id site_id species_id distance bird_count    notes   seen  heard direction\n",
      "npartitions=1                                                                                  \n",
      "                  int64  object     object   object      int64  float64  int64  int64    object\n",
      "                    ...     ...        ...      ...        ...      ...    ...    ...       ...\n",
      "Dask Name: read-csv, 1 tasks>\n",
      "...........................................................................\n",
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(birds_dask.head)\n",
    "print('...........................................................................')\n",
    "print(type(birds_dask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas version of the data frame is `<class 'pandas.core.frame.DataFrame'>` while the Dask version of the dataframe is `<class 'dask.dataframe.core.DataFrame'>`. The Pandas data frame shows up when you look at it, but the Dask data frame appears empty... why? Dask expects to be receiving a huge data set, so it does not automatically display the rows to prevent crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dask functions and methods \n",
    "### (Hint: these are the same as Pandas!)\n",
    "Explore the outputs of the code and consider why they look the way they do\n",
    "<div class=\"run\">\n",
    "    ▶️ <b> Run the cell(s) below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd.Scalar<series-..., dtype=float64>\n",
      "<bound method DataFrame.info of Dask DataFrame Structure:\n",
      "              survey_id site_id species_id distance bird_count    notes   seen  heard direction bird_count_add seen_new\n",
      "npartitions=1                                                                                                          \n",
      "                  int64  object     object   object      int64  float64  int64  int64    object          int64    int64\n",
      "                    ...     ...        ...      ...        ...      ...    ...    ...       ...            ...      ...\n",
      "Dask Name: assign, 9 tasks>\n",
      "<bound method DataFrame.info of Dask DataFrame Structure:\n",
      "              survey_id site_id species_id distance bird_count    notes   seen  heard direction bird_count_add seen_new\n",
      "npartitions=1                                                                                                          \n",
      "                  int64  object     object   object      int64  float64  int64  int64    object          int64    int64\n",
      "                    ...     ...        ...      ...        ...      ...    ...    ...       ...            ...      ...\n",
      "Dask Name: assign, 11 tasks>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bren guest\\.conda\\envs\\eds217\\lib\\site-packages\\dask\\dataframe\\core.py:3858: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('seen', 'int64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "#create small subset of data (first 50 rows) in Dask\n",
    "small_birds_dask = birds_dask.loc[:50]\n",
    "\n",
    "#find the mean of a column in Dask\n",
    "print(birds_dask.bird_count.mean())\n",
    "\n",
    "#add a column to the dask data frame\n",
    "birds_dask['bird_count_add'] = birds_dask.bird_count + 1\n",
    "print(birds_dask.info)\n",
    "\n",
    "#applying functions - create a new column that calculates the seen to heard ratio of birds\n",
    "def seen_calc(seen):\n",
    "    seen_new = (seen * 100)\n",
    "    return seen_new\n",
    "\n",
    "birds_dask['seen_new'] = birds_dask.seen\n",
    "birds_dask.seen.apply(seen_calc)\n",
    "print(birds_dask.info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('eds217')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01298427d7509dc791212f5580d72e2a69e14f859fb9b9009471feff7d08282d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
